# Введение

Конечная бизнес задача: нарисовать по итогу карту DS скиллов. Цель карты помочь начинающим в сфере DS разобраться в ландшафте специальностей, что к чему и как с чем связано. На карту нанесём все курсы КК, что из навыков пре-реквизит и что на выходе.

В рамках этого проекта построить мини-рекомендательную систему. Например, телеграм бот, который говорит тебе какие-то ключевые навыки и ты себя оцениваешь по ним от 1 до 10, а потом он тебя профилирует для профориентации. Можно загрузить список своих скилов.

# Сбор требований к системе

## Функциональные требования

- Разбор вакансий с сайта, напрмер hh
- Возможность просмотра предобработанной интрективной карты близости скилов, специальностей и их связей. Можно рассмотреть фильтрацию или выделение кластеров для уменьшения количества визаулизируемой информации.
- Возможность нанесения скилов обучающих курсов до и после их прохождения Отображение изменений в карте специальностей и изменений зарплаты/востребованности и другой аналитики
- Возможность отправить информацию о скилах и получить профиль профориентации

## Нефункциональные требования
- Изменения вакансий могут обрабатываться в фоне и/или с задержкой
- Парсинг или предобработка не должны нарушить функционирование сервиса
- Интерактивная карта должна бчть отзывчивой для пользователя
- Время формирования рекомендации может составлять несколько секунд, данные по скилам становятся доступными только в момент создания запроса без возможности подготовки рекомендаций заранее
- В рамках проекта высокая нагрузка не рассматривается. Но следует создавать структуру, подходящую для маштабирования.
- Можем собирать аналитическую информацию о запросах пользователей
- Решение должно быть развернуто на облачном сервере и доступно в сети интернет


# Высокоуровневый дизайн
- Используем сайт hh.ru в качестве источника вакансий
- Парсим вакансии с сайта и сохраняем их в хранилище
- Производим предобработку (эмбеддинги, кластеризация, дополнительные фитчи) в фоне или по расписанию. 
- Парсинг и предобработку запускаем по расписанию через AirFlow
- На основе предобраотанных данных строим два сервиса:
	- отображения карты через web-ui, используется дополнительная база обучающих курсов, которые могут быть добавлены в отображение
	- рекомендательную систему с ui-интерфейсом через tg-бот
- Дополнительный сервис аналитики/логирования

![](https://github.com/uberkinder/DS-landscape/raw/main/design/img/HLD_schema.png)

## Выбор баз данных

Нам нужно хранить

- Информацию по вакансиям
	- Если в hh около 10 мил. вакансий, допустим каждая вакансия будет занимать 1Кб, тогда потребуется 10Gb
	- Будем использовать ClickHouse: при расширении проекта анатические возможности ClickHouse могут оказаться полезными, удобный функционал массивов для скилов
- Хранилище признаков: сформированные эмбендинги, параметры кластеров
	- Данных сравнительно мало, чтение происходит чаше записи
	- Будем использвоать Postgres: размер небольшой, особенных требований нет
- Аналитическую инфорацию и информацию мониторинга (при необходимости)
	- Будем использвоать Postgres


## Масштабирование и надежность

Выбранная структура при необходимости позволяет:

- масштабировать отдельные сервисы с использованием балансировщика
- репикацию баз данных вакансий и фитч
- хранить собственный кеш в сервисе поистроения карт

## Развертывание

Планируется подготовить решение в нескольких докер контейнерах, которые будут запущены в облачной среде.


# ML System Design

## Постановка ML задачи

1. Построить эмбеддинги специальностей и скилов, которые будут испольвазоваться как призники в рекомендательной системе в качестве основы построения карт
2. Выполнить кластеризацию вакансий и скилов для использвоания на карте и аналитики
3. Постоить рекомендательную систему подбора специальностей по перечную скилов. Как будет формироваться перечень скилов решим позже. 

## Получение данные
Данные получаем парсингом сайтов с вакансиями. Храним в необработанном виде, чтобы можно было изменять набор признаков и решаемых задач без необходимости перезапуска парсинга

## Процесс обучения, таргет, схема валидации, метрики

Построение эмбеддингов:

1. Берем все данные, строим из них матрицу [Cпециальность] X [Скил], на пересечении будет храниться суммма по всем вакансиям (возможно взвешанная по расположению скилу в списке вакансии)
- Нормируем матрицу одним из способов, потому что каких-то специальностей или скилов может быть значительно больше других
- По этой матрице зная специальность можно получить отранжированный список скилов и наоброт по скилу отранжированный список специальностей - эти списки будут таргетом
- Строим алгоритм, который сформурует эмбеддинги
- На основе эмбеддингов предсказываем отранжированные списки скилов для специальности и специальности для скилов. Считаем метрику nDCG@K по отношению к таргету. Таким образом, мы получим измеряемое значение, сколько мы потеряем при переходе от полной матрицы к сжатым эмбеддингам. Метрика позволит сравнивать алгоритмы и правильно выбрать размерность жмбеддинга. 

Кластеризация

1. Проведем EDA c использованием дополнительных признаков
2. Проведем кластерный анализ
3. Принадлежность к кластеру можем сохранить в виде дополнительного признака
4. При отображении точкек на карте можно отображать кластеры разными цветами. Или выводить только кластеры с индикацией их размера и названия.

Рекомендательная система

1. Будем стремиться сделать рекомендательную систему достаточно простой.
- Пользователь указывает некоторые скилы и оценивает свой уровень по шкале от 1 до 10
- Мы уже имеем систему, которая выдает список специальностей по скилу. Просто считаем взвешенные веса специальностей по заданным скилам. В результат выводим топ-K специальностей
- Используем простую систему логирования запросов и рекомендация для возможной аналитики
- Сохряняем метрики мониторинга (количество запросов, среднее и максимальное время обработки)

Дообучение

1. По расписанию в AirFlow или иному событию (например веб запросу) запускаем парсер новых вакансий
2. Перестраиваем эмбеддинги, считаем метрики, сохраняем их в качестве мониторинга
3. Заменяем значения эмбеддингов и других признаков в таблице базы данных. Считаем что размер данных позволяет выполнить обновление в рамках одной транзакции.

## Обработка признаков

- В нашем решении дополнительные признаки используются только в кластеризации
- Агрегированные данные вакансий в разрезе специальностей (количество, регионы, зарабатная плата)
- Возможно будет интересным разделить специальности по регионам или диапазону заработной платы. 
- На основе расстояния Левенштейна можно попробовать исправить опечатки в названиямх специальностей и скилов





